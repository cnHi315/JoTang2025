#

* \( w_t \)：在时间步 \( t \) 时的模型参数。
* \( g_t \)：在时间步 \(t\) 时计算出的损失函数关于参数的梯度，\( g_t = \nabla f(w_t) \)。
* \( \eta \)：学习率。
* \( \epsilon \)：一个非常小的常数，用于防止除以零（通常为1e-8）。
* \( \beta_1, \beta_2 \)：用于计算梯度一阶矩和二阶矩的指数衰减率。

## ↓

---

### 1. Adam (Adaptive Moment Estimation)

**核心思想：** 结合了 **Momentum** 和 **RMSProp** 的优点。它计算梯度的一阶矩（均值）和二阶矩（未中心化的方差）的指数移动平均数，并利用它们来调整每个参数的学习率。

**运算步骤：**

1. 初始化一阶矩向量 \( m_0 = 0 \)，二阶矩向量 \( v_0 = 0 \)。
2. 在时间步 \( t \)：
   * 计算当前梯度 \( g_t \)。
   * 更新一阶矩估计（类似于动量）：\( m_t = \beta_1 m_{t-1} + (1 - \beta_1) g_t \)
   * 更新二阶矩估计（平方梯度的移动平均）：\( v_t = \beta_2 v_{t-1} + (1 - \beta_2) g_t^2 \)
   * 计算偏差修正后的一阶矩估计：\( \hat{m_t} = \frac{m_t}{1 - \beta_1^t} \)
   * 计算偏差修正后的二阶矩估计：\( \hat{v_t} = \frac{v_t}{1 - \beta_2^t} \)
   * 更新参数：\( w_{t+1} = w_t - \eta \cdot \frac{\hat{m_t}}{\sqrt{\hat{v_t}} + \epsilon} \)

**特点：**

* **自适应学习率：** 每个参数都有自己的学习率，由历史梯度的平方决定。
* **动量：** 通过一阶矩 \( m_t \) 保留了梯度方向的历史信息，有助于在稳定方向加速。
* **偏差校正：** 在训练初期，\( m_t \) 和 \( v_t \) 偏向于0，偏差校正可以缓解这个问题。
* **实践表现优秀：** 通常是深度学习的默认选择之一。

---

### 2. Adagrad (Adaptive Gradient Algorithm)

**核心思想：** 为每个参数自适应地调整学习率，对于不频繁更新的参数（对应稀疏特征）使用较大的学习率，对于频繁更新的参数使用较小的学习率。

**运算步骤：**

1. 初始化累积平方梯度向量 \( G_0 = 0 \)。
2. 在时间步 \( t \)：
   * 计算当前梯度 \( g_t \)。
   * 累积平方梯度：\( G_t = G_{t-1} + g_t^2 \)
   * 更新参数：\( w_{t+1} = w_t - \frac{\eta}{\sqrt{G_t} + \epsilon} \cdot g_t \)

**特点：**

* **单调递减的学习率：** 由于 \( G_t \) 是单调递增的，导致有效学习率 \( \eta / \sqrt{G_t} \) 随时间单调递减，最终可能变得无限小，以至于模型完全停止学习。这是其主要缺点。
* **适合稀疏数据：** 对于出现次数少的特征，其累积梯度平方和较小，因此能获得较大的更新，这很有效。

---

### 3. Adamax (Adam 的无穷范数变体)

**核心思想：** 是 Adam 的一个变体，它提供了一个更简单的、理论上更稳定的范围。它使用 \( L^\infty \) 范数（无穷范数，即取最大值）来替代 Adam 中的 \( L^2 \) 范数用于二阶矩估计。

**运算步骤：**

1. 初始化一阶矩向量 \( m_0 = 0 \)，无穷范数矩向量 \( u_0 = 0 \)。
2. 在时间步 \( t \)：
   * 计算当前梯度 \( g_t \)。
   * 更新一阶矩估计：\( m_t = \beta_1 m_{t-1} + (1 - \beta_1) g_t \)
   * 更新无穷范数矩估计：\( u_t = \max(\beta_2 \cdot u_{t-1}, |g_t|) \) （这里是逐元素比较取最大值）
   * 计算偏差修正后的一阶矩估计：\( \hat{m_t} = \frac{m_t}{1 - \beta_1^t} \)
   * 更新参数：\( w_{t+1} = w_t - \frac{\eta}{u_t + \epsilon} \cdot \hat{m_t} \)

**特点：**

* 相比于 Adam，\( u_t \) 的计算更简单，不需要计算 \( g_t^2 \) 和 \( v_t \)。
* 在某些情况下，尤其是嵌入向量中，可能比 Adam 更稳定。

---

### 4. ASGD (Averaged Stochastic Gradient Descent)

**核心思想：** 运行普通的 SGD，但不会直接使用最后的参数作为结果，而是对训练过程中访问过的所有参数点（或一个子集）取平均值。

**运算步骤：**

1. 运行标准的 SGD 更新：\( w_{t+1} = w_t - \eta \cdot g_t \)
2. 在达到某个预定义的开始轮次后，开始计算参数的滑动平均值：
   \( \bar{w}_{t+1} = \frac{\bar{w}_t \cdot t + w_{t+1}}{t + 1} \)

**特点：**

* **平滑收敛：** 参数平均过程可以平滑优化路径，减少随机梯度的方差，从而可能得到更接近最优解的点。
* **收敛性理论好：** 在某些假设下，其理论收敛性比普通 SGD 更好。
* **内存开销小：** 只需要多存储一份平均后的参数，开销很小。

---

### 5. LBFGS (Limited-memory Broyden–Fletcher–Goldfarb–Shanno)

**核心思想：** 一种拟牛顿法。牛顿法使用损失函数的二阶导数（Hessian 矩阵）来寻找更优的下降方向，但计算和存储完整的 Hessian 矩阵及其逆矩阵对于深度学习来说成本极高。LBFGS 通过只保存最近 m 次的更新历史（向量对）来近似 Hessian 矩阵，从而极大地减少了内存消耗。

**运算方式（两步循环）：**
这是一个复杂的迭代过程，PyTorch 中它通常作为一个黑盒运行。其核心是：

1. 通过两个循环（递归关系）利用最近的 \( s_i = w_{i+1} - w_i \) 和 \( y_i = g_{i+1} - g_i \) 来近似计算搜索方向 \( d_t = -H_t \cdot g_t \)，而无需显式构造 \( H_t \)。
2. 然后执行线搜索，找到一个合适的学习率 \( \alpha \) 来更新参数：\( w_{t+1} = w_t + \alpha d_t \)。

**特点：**

* **二阶信息：** 利用了曲率信息，通常比一阶方法收敛更快、更精确。
* **高内存消耗（相对而言）：** 虽然叫“有限内存”，但对于非常大的模型，存储 m 对向量的开销仍然可能很大。
* **适用于全批量或大批量：** 由于其确定性的性质，在小批量上表现不稳定，通常用于全批量数据集或确定性很强的场景（如强化学习的策略优化）。

---

### 6. RMSprop (Root Mean Square Propagation)

**核心思想：** 由 Geoff Hinton 提出，旨在解决 Adagrad 学习率急剧下降的问题。它使用指数衰减的移动平均来丢弃遥远的历史梯度信息。

**运算步骤：**

1. 初始化累积平方梯度向量 \( E[g^2]_0 = 0 \)。
2. 在时间步 \( t \)：
   * 计算当前梯度 \( g_t \)。
   * 计算指数移动平均的平方梯度：\( E[g^2]_t = \gamma E[g^2]_{t-1} + (1 - \gamma) g_t^2 \)
   * 更新参数：\( w_{t+1} = w_t - \frac{\eta}{\sqrt{E[g^2]_t} + \epsilon} \cdot g_t \)

**特点：**

* **解决 Adagrad 的缺陷：** 学习率不会单调递减到最后消失。
* **是 Adam 的前身：** Adam 可以看作是 RMSprop 加上了动量。
* **在循环神经网络中效果很好。**

---

### 7. Rprop (Resilient Backpropagation)

**核心思想：** 一种适用于全批量学习的优化器。它完全忽略梯度的大小，只使用梯度的符号（方向）来更新参数，而更新步长由独立的、自适应的规则决定。

**运算步骤：**

1. 为每个参数初始化一个单独的更新步长 \( \Delta_t \)。
2. 比较当前梯度 \( g_t \) 和上一个梯度 \( g_{t-1} \) 的符号：
   * 如果 \( g_t \cdot g_{t-1} > 0 \)（方向相同）：增加步长 \( \Delta_t = \eta_+ \cdot \Delta_{t-1} \)（\( \eta_+ > 1 \)），然后沿梯度方向更新：\( w_{t+1} = w_t - \text{sign}(g_t) \cdot \Delta_t \)。
   * 如果 \( g_t \cdot g_{t-1} < 0 \)（方向相反）：说明上一步可能跨过了最优点，因此减小步长 \( \Delta_t = \eta_- \cdot \Delta_{t-1} \)（\( \eta_- < 1 \)），并且**不执行本次更新**（或者回退上一步的更新）。

**特点：**

* **不适用于随机梯度下降（SGD）：** 因为它依赖于连续梯度符号的一致性，而小批量的噪声梯度会破坏这一点。
* **适用于全批量、确定性场景：** 在这种条件下可以非常高效。
* **对学习率不敏感：** 因为它有自己的步长调整机制。

### 总结与选择建议

| 优化器 | 核心思想 | 优点 | 缺点 | 适用场景 |
| :--- | :--- | :--- | :--- | :--- |
| **Adam** | 自适应学习率 + 动量 | 收敛快，鲁棒性好 | 可能泛化不如SGD | **默认推荐**，大多数深度学习任务 |
| **Adagrad** | 累积平方梯度 | 适合稀疏数据 | 学习率消失 | 自然语言处理，推荐系统（稀疏特征） |
| **Adamax** | Adam的无穷范数版 | 理论上更稳定 | 实践中使用较少 | 可作为Adam的替代尝试 |
| **ASGD** | 参数平均 | 平滑收敛，理论好 | 收敛可能较慢 | 需要稳定解时，如强化学习 |
| **LBFGS** | 近似二阶方法 | 收敛精度高 | 内存高，不适于小批量 | 全批量学习，小规模问题 |
| **RMSprop** | 指数衰减平方梯度 | 解决Adagrad问题 | 被Adam超越 | **RNN**的经典选择 |
| **Rprop** | 符号+自适应步长 | 全批量下高效 | **不能用于小批量** | 全批量学习 |

**一般建议：**

* **首选 Adam**，因为它通常在各种任务上都能快速收敛且效果不错。
* 对于 **循环神经网络（RNN/LSTM）**，**RMSprop**  historically 是一个好选择，不过现在 Adam 也广泛使用。
* 如果你追求 **最佳泛化性能**，并且不介意更长的训练时间，可以尝试使用 **带动量的SGD** 或 **ASGD**。
* **LBFGS** 和 **Rprop** 主要用于全批量、确定性的优化问题，在标准的随机深度学习训练中很少使用。
