# task 1

## 概念理解

---

### 1. 什么是多层感知机（MLP）？其结构是怎样的？

* **什么是MLP**：**多层感知机（Multilayer Perceptron）** 是最基础、也是最经典的一种**前馈神经网络**结构。它可以理解为一个函数，能够将一组输入值映射到一组输出值。它的强大之处在于能够学习和表示非常复杂的非线性关系。
* **结构**：MLP 的结构是分层的，通常由三种类型的层组成：
    1. **输入层（Input Layer）**：接收原始数据。层中的每个“神经元”代表输入数据的一个**特征**（Feature）。它不做任何计算，只是将数据传递到下一层。
    2. **隐藏层（Hidden Layers）**：介于输入层和输出层之间的所有层。这些层是“隐藏”的，因为我们在网络的输入和输出中看不到它们的值。每个隐藏层由多个**神经元**（或称为单元）组成，每个神经元都会对前一层的输出进行加权求和并施加**激活函数**。一个MLP可以有一个或多个隐藏层。
    3. **输出层（Output Layer）**：产生最终的预测结果。神经元的数量和使用的激活函数取决于任务类型（例如，二分类问题通常用1个神经元+Sigmoid，多分类问题用多个神经元+Softmax，回归问题用线性激活函数）。

**核心特点**：层与层之间是**全连接**的，意思是某一层的每一个神经元都与下一层的每一个神经元相连。

---

### 2. 数据在神经网络中扮演哪些角色？（数据集的 split 和处理）

数据是神经网络的“燃料”和“老师”。模型通过数据来学习内在规律。

* **数据集的划分（Split）**：
  * **训练集（Training Set）**：用于**训练模型**，即调整模型的参数（权重和偏置）。模型从这部分数据中学习模式。
  * **验证集（Validation Set）**：用于在训练过程中**评估模型**，调整**超参数**（如学习率、层数），以及进行**早停**（Early Stopping）以防止过拟合。它不参与参数更新。
  * **测试集（Test Set）**：用于在模型训练和调优完全结束后，**最终评估**模型的泛化能力（在新数据上的表现）。测试集在整个训练过程中绝对不能使用，以保证评估的公正性。

* **数据处理**：包括数据清洗、缺失值处理、**归一化/标准化**（将特征缩放到相近的数值范围，加速训练）、数据增强等。

* **噪声（Noise）**：数据中随机的、无意义的波动或错误。它会使模型的学习变得困难，因为模型可能会去学习这些噪声而不是真正的规律，导致过拟合。在 Playground 中，你可以手动调节数据点的“噪声”水平。

* **特征（Feature）**：输入的变量，是描述数据点的属性。例如，在预测房价时，特征可以是面积、卧室数量、地理位置等。在 Playground 的二维数据中，特征就是每个点的 X1 和 X2 坐标。

* **标签（Label）**：我们希望预测的输出值或类别。在监督学习中，每个数据点都对应一个标签。例如，房价本身（回归任务），或“猫/狗”（分类任务）。在 Playground 中，标签就是数据点的颜色（蓝色或橙色）。

* **Batch Size（批大小）**：
  * **是什么**：一次迭代（Iteration）中用于更新参数所使用的训练样本数量。
  * **为什么能提高速度**：
      1. **硬件并行化**：现代GPU/CPU擅长并行计算。一次性计算一个批量的数据的损失和梯度，比一个一个样本计算要高效得多，能更好地利用硬件资源。
      2. **更稳定的梯度**：相比于单个样本，一个批量的梯度是多个样本梯度的平均值，能提供一个对整体数据梯度更稳定、噪音更少的估计，从而使训练过程更平稳。

---

### 3. 神经元是什么？

神经网络中的**神经元**是模仿生物神经元的基本计算单元。

* **功能**：它接收来自前一层其他神经元的输入信号（x1, x2, ...），为每个输入分配一个**权重**（w1, w2, ...），计算所有输入的**加权和**，再加上一个**偏置**（bias, b），最后将结果通过一个**激活函数**（f）进行非线性变换，产生一个输出信号。
* **数学表达**：`输出 = f(w1*x1 + w2*x2 + ... + b)`

---

### 4. 什么是激活函数？常见的激活函数有哪些？什么叫“非线性表达能力”？

* **激活函数（Activation Function）**：作用于神经元输出的函数，用于引入**非线性**因素。如果没有激活函数，无论神经网络有多少层，最终都等价于一个线性模型，无法解决复杂问题（如Playground中的异或问题）。
* **常见激活函数**：
  * **Sigmoid**：将输出压缩到(0, 1)之间，常用于二分类的输出层。
  * **Tanh**：将输出压缩到(-1, 1)之间，均值是0，收敛性通常比Sigmoid好。
  * **ReLU（Rectified Linear Unit）**：`f(x) = max(0, x)`。这是目前最常用的激活函数，因为它能有效缓解梯度消失问题，计算简单，加速收敛。
  * **Softmax**：常用于多分类问题的输出层，它将所有神经元的输出转换为概率分布（每个值在0到1之间，所有值之和为1）。
* **非线性表达能力**：指的是模型拟合和表示复杂非线性函数或决策边界的能力。通过堆叠带有激活函数的隐藏层，神经网络可以组合出极其复杂的非线性函数，从而处理像图像、语言这样的高度复杂的数据。

---

### 5. 什么是计算图？它和数据结构/离散数学中学的图有什么区别？怎么构建计算图？

* **计算图（Computational Graph）**：是一种用于描述数学运算的有向图，是深度学习框架（如TensorFlow, PyTorch）的核心概念。图中的**节点**代表变量（输入、参数、中间结果），**边**代表运算（加法、乘法、激活函数等）。
* **区别**：
  * **数据结构/离散数学中的图**：更通用，研究的是节点和边的抽象关系（如连通性、最短路径）。
  * **计算图**：是一种特殊的有向无环图（DAG），每个节点都有明确的**数学运算**含义，并且具有**方向性**（从前向传播）和**可微分性**（为反向传播服务）。
* **如何构建**：从输入开始，根据模型的运算过程（前向传播公式）一步步画出节点和边。例如，`z = w*x + b` 可以分解为 `mul_node = w * x` 和 `add_node = mul_node + b` 两个节点。

---

### 6. 怎么计算 MLP 的参数？什么是超参数？MLP 有哪些超参数？

* **计算参数数量**：参数主要指**权重（Weights）** 和**偏置（Biases）**。
  * **权重**：对于两个相邻层，如果前一层有 `m` 个神经元，后一层有 `n` 个神经元，那么它们之间的权重矩阵就有 `m * n` 个参数。
  * **偏置**：下一层的每个神经元都有一个偏置，所以是 `n` 个参数。
  * **总参数**：将网络中所有这样的权重和偏置数量相加。例如，一个 输入层2 -> 隐藏层3 -> 输出层1 的网络，参数 = (2*3 + 3) + (3*1 + 1) = 13个。

* **超参数（Hyperparameters）**：
  * **是什么**：在模型训练**之前**就由人工设定的参数，**不能**通过训练过程学习得到。
  * **MLP的常见超参数**：
    * 网络结构：隐藏层的**数量**、每层的**神经元数量**。
    * 学习率（Learning Rate）。
    * 批大小（Batch Size）。
    * 迭代次数（Epochs）。
    * 优化器类型（Optimizer）。
    * 激活函数的选择。
    * 正则化系数（λ）。

---

### 7. 什么是隐藏层（hidden layers）？它为什么叫这个名字？

* **隐藏层**：输入层和输出层之间的所有网络层。
* **名字由来**：因为训练数据只提供了输入和预期的输出（标签），而这些中间层的输出在训练过程中并**不被“直接观察”或“直接监督”**。我们看不到它们具体计算出了什么，它们就像是模型的“内部思考过程”，是隐藏的。它们的价值在于自动从数据中提取和组合更有用的**特征表示**。

---

### 8. 什么是损失函数？什么任务用什么损失函数？

* **损失函数（Loss Function）**：也称为代价函数（Cost Function），用于**衡量模型预测值（y_pred）与真实标签（y_true）之间的差距**。训练模型的最终目标就是最小化这个损失函数。
* **常见损失函数**：
  * **均方误差（MSE）**：最常用于**回归**问题（预测一个连续值）。
  * **交叉熵损失（Cross-Entropy Loss）**：最常用于**分类**问题（预测一个离散类别）。二分类常用二元交叉熵，多分类常用分类交叉熵。

---

### 9. 前向传播是什么？梯度是什么？学习率是什么？反向传播是什么？有哪些常见的优化器？

* **前向传播（Forward Propagation）**：数据从输入层流向输出层的过程。输入数据经过每一层的加权求和和激活函数，最终得到预测输出。
* **梯度（Gradient）**：一个向量，表示损失函数对于每个参数的**偏导数**。它指明了损失函数上升最快的方向。为了最小化损失，我们需要沿着梯度的**反方向**（即下降最快的方向）更新参数。
* **学习率（Learning Rate）**：一个超参数，控制每次参数更新时的**步长**。学习率太小，收敛慢；学习率太大，可能会在最优点附近震荡甚至发散。
* **反向传播（Backpropagation）**：**核心训练算法**。它首先通过前向传播计算预测值和损失，然后利用**链式法则**从输出层到输入层**反向**计算损失函数对每个参数的梯度。有了梯度，优化器就可以更新参数了。
* **常见优化器（Optimizer）**：用于根据梯度更新参数的算法。
  * **SGD（随机梯度下降）**：最基本的形式。
  * **SGD with Momentum**：引入“动量”，加速收敛并减少震荡。
  * **Adam**：目前最常用和最受欢迎的优化器，它结合了Momentum和自适应学习率的优点。

---

### 10. 归一化是什么？正则化是什么？

* **归一化（Normalization）**：
  * **是什么**：一种**数据预处理**技术，将数据的特征 scale 到统一的区间（如[0,1]或均值为0方差为1），消除不同特征因量纲和范围不同而带来的差异。
  * **为什么**：加速训练收敛，提高模型性能。在 Playground 中，你可以看到数据是分布在一个单位圆内的，这就是一种归一化。

* **正则化（Regularization）**：
  * **是什么**：一种用于**防止模型过拟合**的技术，通过在损失函数中添加一个与模型复杂度相关的**惩罚项**，来约束模型参数的大小，鼓励模型变得简单。
  * **常见方法**：
    * **L2正则化**：惩罚权重向量的平方和，使权重更趋向于小而分散的值。
    * **L1正则化**：惩罚权重向量的绝对值之和，容易产生稀疏权重（很多权重为0）。
    * **Dropout**：在训练过程中随机“丢弃”一部分神经元，强迫网络不依赖任何单个神经元，增强鲁棒性。

---

### 11. 什么是欠拟合？什么是过拟合？

这两个概念描述了模型在训练数据和新数据上表现的好坏。

* **欠拟合（Underfitting）**：
  * **表现**：模型在**训练集**上的表现就很差（高训练误差）。它无法捕捉数据中的基本规律。
  * **原因**：模型太简单（例如层数太少、神经元太少），或者训练时间不够。
  * **类比**：学生连课本上的习题都做不对。
  * **解决方法**：增加模型复杂度、增加特征、减少正则化、延长训练时间。

* **过拟合（Overfitting）**：
  * **表现**：模型在**训练集**上表现非常好（极低的训练误差），但在**验证集/测试集**上表现很差（高验证误差）。模型过度学习了训练数据中的细节和噪声，导致泛化能力差。
  * **原因**：模型过于复杂，或者训练数据太少。
  * **类比**：学生把课本习题和答案死记硬背下来，但遇到新题就不会做了。
  * **解决方法**：获取更多数据、降低模型复杂度、采用正则化（L2, Dropout）、早停（Early Stopping）。
